@article{schramowskiLanguageModelsHave2021,
  title = {Language {{Models}} Have a {{Moral Dimension}}},
  author = {Schramowski, P. and Turan, Cigdem and Andersen, Nico and Rothkopf, C. and Kersting, K.},
  year = {2021},
  journal = {undefined},
  urldate = {2022-08-07},
  abstract = {Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, the capabilities of the moral direction for guiding LMs towards producing normative text are demonstrated and demonstrated on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2. Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended state of the art for many NLP tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show that recent improvements of LMs also store ethical and moral norms of the society and actually bring a ``moral direction'' to surface. In this study, we show that these norms can be captured geometrically by a direction, which can be computed, e.g., by a PCA, in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts. Furthermore, this provides a path for attenuating or even preventing toxic degeneration in LMs. Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the moral direction for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2. We used a significance level of 5\% in the analysis. Samples with missing values, i.e. where the participants failed to respond within five seconds, were},
  langid = {english},
  file = {/Users/gabe/Zotero/storage/2582FTEB/1bbbe12b060f54a97a4e81fd23cbb7932ff9de53.html}
}


@article{hammerlMultilingualLanguageModels2022,
  title = {Do {{Multilingual Language Models Capture Differing Moral Norms}}?},
  author = {Hammerl, Katharina and Deiseroth, Björn and Schramowski, P. and Libovický, Jindřich and Fraser, Alexander and Kersting, K.},
  date = {2022},
  journaltitle = {undefined},
  doi = {10.48550/arXiv.2203.09904},
  url = {https://www.semanticscholar.org/paper/Do-Multilingual-Language-Models-Capture-Differing-Hammerl-Deiseroth/bae87fc918564d39990423340d58ab7162c18931},
  urldate = {2022-08-08},
  abstract = {The initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones, but it is not yet clear to what extent these moral norms differ between languages. Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages. Recent work demonstrated large pre-trained language models (PLM) obtain symbolic, relational [12] but also commonsense knowledge [5]. Further, West et al. [17] showed that one is able to extract the commonsense knowledge from the large, general language model GPT-3 [2] via symbolic knowledge distillation. This encoded “knowledge” includes information of our society reflecting ethical},
  langid = {english},
  file = {/Users/gabe/Zotero/storage/USGBWWUM/Hammerl et al. - 2022 - Do Multilingual Language Models Capture Differing .pdf;/Users/gabe/Zotero/storage/CLZX58D3/bae87fc918564d39990423340d58ab7162c18931.html}
}

@article{jiangCanMachinesLearn2021,
  title = {Can {{Machines Learn Morality}}? {{The Delphi Experiment}}},
  shorttitle = {Can {{Machines Learn Morality}}?},
  author = {Jiang, Liwei and Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Liang, Jenny and Dodge, Jesse and Sakaguchi, Keisuke and Forbes, Maxwell and Borchardt, Jon and Gabriel, Saadia and Tsvetkov, Yulia and Etzioni, Oren and Sap, Maarten and Rini, Regina and Choi, Yejin},
  date = {2021-10-14},
  eprint = {2110.07574},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.07574},
  url = {http://arxiv.org/abs/2110.07574},
  urldate = {2022-08-13},
  abstract = {As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it. To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense. Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/gabe/Zotero/storage/L7IRMVMV/Jiang et al. - 2021 - Can Machines Learn Morality The Delphi Experiment.pdf;/Users/gabe/Zotero/storage/YSARXJ3N/2110.html}
}

@inproceedings{fraserDoesMoralCode2022,
  title = {Does {{Moral Code}} Have a {{Moral Code}}? {{Probing Delphi}}'s {{Moral Philosophy}}},
  shorttitle = {Does {{Moral Code}} Have a {{Moral Code}}?},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Trustworthy Natural Language Processing}} ({{TrustNLP}} 2022)},
  author = {Fraser, Kathleen C. and Kiritchenko, Svetlana and Balkir, Esma},
  year = {2022},
  month = jul,
  pages = {26--42},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, U.S.A.},
  doi = {10.18653/v1/2022.trustnlp-1.3},
  urldate = {2022-08-13},
  abstract = {In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -- and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.},
  file = {/Users/gabe/Zotero/storage/HX6V2DNW/Fraser et al. - 2022 - Does Moral Code have a Moral Code Probing Delphi'.pdf}
}

@inproceedings{mutluBotsHaveMoral2020,
  title = {Do {{Bots Have Moral Judgement}}? {{The Difference Between Bots}} and {{Humans}} in {{Moral Rhetoric}}},
  shorttitle = {Do {{Bots Have Moral Judgement}}?},
  booktitle = {2020 {{IEEE}}/{{ACM International Conference}} on {{Advances}} in {{Social Networks Analysis}} and {{Mining}} ({{ASONAM}})},
  author = {Mutlu, Ece {\c C}i{\u g}dem and Oghaz, Toktam and T{\"u}t{\"u}nc{\"u}ler, Ege and Garibay, Ivan},
  year = {2020},
  month = dec,
  pages = {222--226},
  issn = {2473-991X},
  doi = {10.1109/ASONAM49781.2020.9381386},
  abstract = {Understanding moral foundations can yield powerful results in terms of perceiving the intended meaning of the text data, as the concept of morality provides additional information on the unobservable characteristics of information processing and non-conscious cognitive processes. Considering that moral values vary significantly across cultures and yet many recurrent themes are observed and that each culture builds its societal and ideological narratives on top of its moral virtues, an enhanced understanding of morality can prove to be a valuable tool in deterring disinformation narratives by adversaries. Therefore, we investigate the evolution of latent moral loadings over time and across different sub-narratives on human and bot-generated tweets. For this purpose, we analyze the Syrian White Helmets-related tweets from April 1st, 2018 to April 30th, 2019. For the operationalization and quantification of moral rhetoric in tweets, we use Moral Foundations Dictionary in which five psychological dimensions (Harm/Care, Subversion/Authority, Cheating/Fairness, Betrayal/Loyalty and Degradation/Purity) are considered. Our results present the significant differences between the strength and patterns of moral rhetoric for human and bot-generated content on Twitter.},
  keywords = {Bot,Ethics,Head,latent semantic analysis,Loading,MFT,moral foundations,moral judgement,Rhetoric,Safety,Social networking (online),Tools,Twitter,White Helmets},
  file = {/Users/gabe/Zotero/storage/C3UBQAET/9381386.html}
}
